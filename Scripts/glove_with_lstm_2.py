# -*- coding: utf-8 -*-
"""Glove with LSTM #2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_ByDPunlGg8xZeKD3xUWdkW3aCKBRrd
"""

import pandas as pd

train_df = pd.read_csv('Train_Dataset.csv')

train_df.head()

train_df['sarcastic'].value_counts() # label changed to sarcastic

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip

!unzip glove.6B.zip #shows the different embeddings downloaded from glow, using 50 dimensional word embeddings for this exercise

import numpy as np
words = dict()

def add_to_dict(d, filename):
    with open(filename, 'r') as f:
        for line in f.readlines():
            line = line.split(' ')

            try:
                d[line[0]] = np.array(line[1:], dtype=float) # separates word [line[0]] from its embeddings [line[1:]]
            except:
                continue
add_to_dict(words, 'glove.6B.50d.txt')
words

len(words)

import nltk

nltk.download('wordnet')

tokenizer = nltk.RegexpTokenizer(r"\w+") #comes with built in preprocessing steps
#removes special characters

tokenizer.tokenize('@user this is #sarcasm')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

lemmatizer.lemmatize('feet')

def message_to_token_list(s):
  tokens = tokenizer.tokenize(s)
  lowercased_tokens = [t.lower() for t in tokens]
  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]
  useful_tokens = [t for t in lemmatized_tokens if t in words]

  return useful_tokens

message_to_token_list('@user this is a sarcastic tweet at https')

def message_to_word_vectors(message, word_dict=words):
  processed_list_of_tokens = message_to_token_list(message)

  vectors = [] # initialize empty array

  for token in processed_list_of_tokens:
    if token not in word_dict:
      continue

    token_vector = word_dict[token]
    vectors.append(token_vector)


  return np.array(vectors, dtype=float)

message_to_word_vectors('@user this is a sarcastic tweet at https').shape

#this line is splitting the data for training, validating and testing. changed slightly for testing
train_df = train_df.sample(frac=1, random_state=1)

train_df.reset_index(drop=True, inplace=True)

split_index = int(len(train_df) * 0.7)

train_df, val_df = train_df[:split_index], train_df[split_index:]

test = pd.read_csv('Train_Dataset.csv')

test = test.sample(frac=1, random_state=1)
test.reset_index(drop=True, inplace=True)

test_df = test

len(train_df), len(val_df), len(test_df) # not ideal sizes but

test_df #id= Unnamed: label= sarcastic and tweet=tweet~

def df_to_X_y(dff):
  y = dff['sarcastic'].to_numpy().astype(int)

  all_word_vector_sequences = []

  for message in dff['tweet']:
    message_as_vector_seq = message_to_word_vectors(message)

    if message_as_vector_seq.shape[0] == 0:
      message_as_vector_seq = np.zeros(shape=(1, 50))

    all_word_vector_sequences.append(message_as_vector_seq)

  return all_word_vector_sequences, y

X_train, y_train = df_to_X_y(train_df)

print(len(X_train), len(X_train[0]))

print(len(X_train), len(X_train[2]))

sequence_lengths = []

for i in range(len(X_train)):
  sequence_lengths.append(len(X_train[i]))

import matplotlib.pyplot as plt

plt.hist(sequence_lengths)

pd.Series(sequence_lengths).describe()

from copy import deepcopy

def pad_X(X, desired_sequence_length=57):
  max_sequence_length= max(len(x) for x in X) # added by chat
  X_copy = deepcopy(X)

  for i, x in enumerate(X):
    #x_seq_len = x.shape[0]
    #sequence_length_difference = desired_sequence_length - x_seq_len
    sequence_length_difference = max_sequence_length - len(x)

    #added next line from chat to ensure no negative values:
    if sequence_length_difference > 0:
      pad = np.zeros(shape=(sequence_length_difference, 50))
      X_copy[i] = np.concatenate([x, pad])

  return np.array(X_copy).astype(float)

X_train = pad_X(X_train)

X_train.shape

y_train.shape

import pandas as pd

# Display the first few rows of val_df
print("First few rows of val_df:")
print(val_df.head())

# Check data types of columns in val_df
print("\nData types of columns in val_df:")
print(val_df.dtypes)

# Inspect specific columns containing text data
text_columns = ['tweet']  # Replace with actual column names
for col in text_columns:
    print(f"\nInspecting column '{col}':")
    print(val_df[col].head())

X_val, y_val = df_to_X_y(val_df)
X_val = pad_X(X_val)

X_val.shape, y_val.shape

X_test, y_test = df_to_X_y(test_df)
X_test = pad_X(X_test)

X_test.shape, y_test.shape