#for f1 score use scikit-learns classification_report
import sklearn
from sklearn.metrics import classification_report

"""
Example: Given
print(classification_report(y_train, test_predictions))

where test_predictions = (best_model.predict(X_train) > 0.5).astype(int)

Returns:
 precision    recall  f1-score   support

           0       0.84      0.74      0.78      1823
           1       0.41      0.56      0.48       604

    accuracy                           0.69      2427
   macro avg       0.63      0.65      0.63      2427
weighted avg       0.73      0.69      0.71      2427

precision, recall, f1-score and support 
as well as macrco avg and weighted avg
Support - is the number of samples each metric was calculated on in our test data set

Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html

"""

# balanced accuracy score
from sklearn.metrics import balanced_accuracy_score

"""
sklearn.metrics.balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)
print(balanced_accuracy_score(y_train, test_predictions))
--0.6498058291223622

Documentatin: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html

"""


#weighted cross loss entropy

import tensorflow as tf
tf.nn.weighted_cross_entropy_with_logits -- computes a weighted cross entropy

tf.nn.weighted_cross_entropy_with_logits(labels, logits, pos_weight, name=None)

"""
Documentation: https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits

labels:  0 0r 1, i.e. sarcastic or not


logits: Logits are the raw, unnormalized predictions generated by the model before applying any activation function. In a binary classification scenario, logits are often a single real-valued number per sample, representing the model's confidence that the sample belongs to the positive class. Logits can be positive or negative, and they are usually fed into an activation function (like the sigmoid function) to obtain probabilities.

pos_weight: parameter to be adjusted, a 'trade-off' between recall and precision
Chat: 
This is a scalar value (or a tensor that can be broadcasted to the shape of logits) that is used to weight the loss associated with positive examples (samples from the positive class). It allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.
If pos_weight is greater than 1, it increases the weight of positive examples, thereby placing more emphasis on correctly classifying positive instances (which can increase recall at the expense of precision).
If pos_weight is less than 1, it decreases the weight of positive examples, which can increase precision at the expense of recall.
If pos_weight equals 1, it means that positive and negative examples are equally weighted (standard binary cross-entropy).

This means our fuction has to take pos_weight as a param
"""
#Example with our model:
import tensorflow as tf
import random
import pandas as pd
from transformers import DistilBertTokenizer
from Data_Preprocessing import preprocess_type_II, preprocess_type_III, \
    preprocess_type_I, preprocess_type_IV_A, preprocess_type_IV_B
from Evaluation import f1_m


def weighted_binary_cross_entropy_with_logits(pos_weight):
    def loss_func(labels, logits):
        return tf.nn.weighted_cross_entropy_with_logits(labels=labels, logits=logits, pos_weight=pos_weight)
    return loss_func


def dataset_embedding(dataset_path, tokenizer, preproc_type=None, batch_size=32):

    dataset = pd.read_csv(dataset_path)[["tweet", "sarcastic"]]
    dataset = dataset[dataset['tweet'].notna()]

    # Conditional preprocessing based on preproc_type
    if preproc_type == 'PreprocI':
        dataset['tweet'] = dataset['tweet'].apply(preprocess_type_I)
    elif preproc_type == 'PreprocII':
        dataset['tweet'] = dataset['tweet'].apply(preprocess_type_II)
    elif preproc_type == 'PreprocIII':
        dataset['tweet'] = dataset['tweet'].apply(preprocess_type_III)
    elif preproc_type == 'PreprocIV_A':
        dataset['tweet'] = dataset['tweet'].apply(preprocess_type_IV_A)
    elif preproc_type == 'PreprocIV_B':
        dataset['tweet'] = dataset['tweet'].apply(preprocess_type_IV_B)

    tokenized_tweets = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet)) for tweet in dataset['tweet']]
    tweets_with_len = [[tweet, dataset['sarcastic'].iloc[i], len(tweet)] for i, tweet in enumerate(tokenized_tweets)]
    random.Random(42).shuffle(tweets_with_len)
    tweets_with_len.sort(key=lambda x: x[2])
    sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]  # remove tweet len

    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.int32))

    return processed_dataset.padded_batch(batch_size, padded_shapes=((None,), ()))


def prepare_datasets(train_path, test_path, preproc_type):
    # replaced their tokenizer with distil bert-base-uncased
    model_name = 'distilbert-base-uncased'

    # Initialize the tokenizer and model specifically for DistilBERT
    tokenizer = DistilBertTokenizer.from_pretrained(model_name)
    # model = TFDistilBertModel.from_pretrained(model_name)

    dataset_train = dataset_embedding(train_path, tokenizer, preproc_type)
    dataset_test = dataset_embedding(test_path, tokenizer, preproc_type)

    return dataset_train, dataset_test, tokenizer

# Choosing model type to train
def create_model(model_type, tokenizer):
    if model_type == 'BiLSTM':
        model = tf.keras.Sequential([
            tf.keras.layers.Embedding(len(tokenizer.vocab), 32),
            tf.keras.layers.LSTM(32),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
    elif model_type == 'LSTM':
        model = tf.keras.Sequential([
            tf.keras.layers.Embedding(len(tokenizer.vocab), 32),
            tf.keras.layers.LSTM(32, return_sequences=True),
            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32, activation='relu')),
            tf.keras.layers.LSTM(32, return_sequences=True),
            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32, activation='relu')),
            tf.keras.layers.LSTM(32),
            tf.keras.layers.Dense(8, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Modify loss function to use weighted binary cross-entropy
    if model_type in ['BiLSTM', 'LSTM']:
        model.compile(loss=weighted_binary_cross_entropy_with_logits(pos_weight=1.5),
                      optimizer='adam', metrics=['accuracy', f1_m])
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    return model


#confusion matrix code and plotting code
from sklearn.metrics import confusion_matrix
#confusion matrix
print(confusion_matrix(y_test, y_preds))

#visualisation
#import seaborn
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(font_scale=1.5) #increase font size

def plot_conf_mat(y_test, y_preds):
    """
    Plots a confusion matrix using Seaborm's heatmap().
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sns.heatmap(confusion_matrix(y_test, y_preds),
                     annot=True,#annotate the boxes
                     cbar=False)
    plt.xlabel("predicted Label") # predictions go on the x-axis
    plt.ylabel("True Labels") # true labels go on the y-axis
    
    bottom, top = ax.get_ylim()
    ax.set_ylim(bottom + 0.5, top-0.5)
plot_conf_mat(y_test, y_preds)