# -*- coding: utf-8 -*-
"""Glove with LSTM #1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NArkQ-zfOdQ9YWmtIy9H30frSylysMSO
"""

import pandas as pd

train_df = pd.read_csv('Train_Dataset.csv')

train_df.head()

train_df['sarcastic'].value_counts() # label changed to sarcastic

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip

!unzip glove.6B.zip #shows the different embeddings downloaded from glow, using 50 dimensional word embeddings for this exercise

import numpy as np
words = dict()

def add_to_dict(d, filename):
    with open(filename, 'r') as f:
        for line in f.readlines():
            line = line.split(' ')

            try:
                d[line[0]] = np.array(line[1:], dtype=float) # separates word [line[0]] from its embeddings [line[1:]]
            except:
                continue
add_to_dict(words, 'glove.6B.50d.txt')
#words

len(words)

import nltk
nltk.download('wordnet')

tokenizer = nltk.RegexpTokenizer(r"\w+") #comes with built in preprocessing steps
#removes special characters

tokenizer.tokenize('@user this is #sarcasm')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

lemmatizer.lemmatize('feet')

def message_to_token_list(s):
  tokens = tokenizer.tokenize(s)
  lowercased_tokens = [t.lower() for t in tokens]
  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]
  useful_tokens = [t for t in lemmatized_tokens if t in words]

  return useful_tokens

message_to_token_list('@user this is a sarcastic tweet at https')

def message_to_word_vectors(message, word_dict=words):
  processed_list_of_tokens = message_to_token_list(message)

  vectors = [] # initialize empty array

  for token in processed_list_of_tokens:
    if token not in word_dict:
      continue

    token_vector = word_dict[token]
    vectors.append(token_vector)


  return np.array(vectors, dtype=float)

message_to_word_vectors('@user this is a sarcastic tweet at https').shape

train_df = train_df.sample(frac=1, random_state=1)
train_df.reset_index(drop=True, inplace=True)

split_index_1 = int(len(train_df) * 0.7)
split_index_2 = int(len(train_df) * 0.85)

train_df, val_df, test_df = train_df[:split_index_1], train_df[split_index_1:split_index_2], train_df[split_index_2:]

len(train_df), len(val_df), len(test_df)

test_df

def df_to_X_y(dff):
  y = dff['sarcastic'].to_numpy().astype(int)

  all_word_vector_sequences = []

  for message in dff['tweet']:
    message_as_vector_seq = message_to_word_vectors(message)

    if message_as_vector_seq.shape[0] == 0:
      message_as_vector_seq = np.zeros(shape=(1, 50))

    all_word_vector_sequences.append(message_as_vector_seq)

  return all_word_vector_sequences, y

X_train, y_train = df_to_X_y(train_df)

print(len(X_train), len(X_train[0]))

print(len(X_train), len(X_train[2]))

sequence_lengths = []

for i in range(len(X_train)):
  sequence_lengths.append(len(X_train[i]))

import matplotlib.pyplot as plt

plt.hist(sequence_lengths)

pd.Series(sequence_lengths).describe()

from copy import deepcopy
"""
def pad_X(X, desired_sequence_length=57):
  X_copy = deepcopy(X)

  for i, x in enumerate(X):
    x_seq_len = x.shape[0]
    sequence_length_difference = desired_sequence_length - x_seq_len

    pad = np.zeros(shape=(sequence_length_difference, 50))

    X_copy[i] = np.concatenate([x, pad])

  return np.array(X_copy).astype(float)

  from copy import deepcopy
"""
def pad_X(X, desired_sequence_length=57):
  max_sequence_length= max(len(x) for x in X) # added by chat
  X_copy = deepcopy(X)

  for i, x in enumerate(X):
    #x_seq_len = x.shape[0]
    #sequence_length_difference = desired_sequence_length - x_seq_len
    sequence_length_difference = max_sequence_length - len(x)

    #added next line from chat to ensure no negative values:
    if sequence_length_difference > 0:
      pad = np.zeros(shape=(sequence_length_difference, 50))
      X_copy[i] = np.concatenate([x, pad])

  return np.array(X_copy).astype(float)

X_train = pad_X(X_train)

X_train.shape

y_train.shape

"""X_val, y_val = df_to_X_y(val_df)
X_val = pad_X(X_val)

X_val.shape, y_val.shape"""

X_test, y_test = df_to_X_y(test_df)
X_test = pad_X(X_test)

X_test.shape, y_test.shape

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

model = Sequential([])

model.add(layers.Input(shape=(63, 50))) # so we definitely have a shape/pad issue in our data
model.add(layers.LSTM(64, return_sequences=True))
model.add(layers.Dropout(0.2))
model.add(layers.LSTM(64, return_sequences=True))
model.add(layers.Dropout(0.2))
model.add(layers.LSTM(64, return_sequences=True))
model.add(layers.Dropout(0.2))
model.add(layers.Flatten())
model.add(layers.Dense(1, activation='sigmoid'))

model.summary()

from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC
from tensorflow.keras.callbacks import ModelCheckpoint

cp = ModelCheckpoint('model/', save_best_only=True)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss=BinaryCrossentropy(),
              metrics=['accuracy', AUC(name='auc')])

frequencies = pd.value_counts(train_df['sarcastic'])

frequencies

weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1]}
weights

# Pad sequences in the training data to a length of 57 tokens
X_train_padded = pad_X(X_train, desired_sequence_length=57)

# Check the shape of the padded training data
print("Shape of X_train_padded:", X_train_padded.shape)

# Pad sequences in the training data to a length of 57 tokens
X_train_padded = pad_X(X_train, desired_sequence_length=57)

X_train_padded.shape

model.fit(X_train, y_train, validation_data=(X_train, y_train), epochs=20, callbacks=[cp], class_weight=weights) #changed val to train as error

from tensorflow.keras.models import load_model

best_model = load_model('model/')

test_predictions = (best_model.predict(X_train) > 0.5).astype(int)

from sklearn.metrics import classification_report

print(classification_report(y_train, test_predictions))

"""test_predictions = (best_model.predict(X_test) > 0.5).astype(int)

from sklearn.metrics import classification_report

print(classification_report(y_test, test_predictions))"""
#shape error with test_df

