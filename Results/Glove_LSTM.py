# -*- coding: utf-8 -*-
"""Glove_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oZ2tiTfkNmBdoDo-yapZkXyGIuEiWNtT
"""

import pandas as pd

train_df = pd.read_csv('train.En.csv')

train_df.head()

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
!unzip glove.6B.zip #shows the different embeddings downloaded from glove, using 50 dimensional word embeddings for this project

#creating a dictionary and adding the glove embeddings
import numpy as np
words = dict()

def add_to_dict(d, filename):
    with open(filename, 'r') as f:
        for line in f.readlines():
            line = line.split(' ')

            try:
                d[line[0]] = np.array(line[1:], dtype=float) # separates word [line[0]] from its embeddings [line[1:]]
            except:
                continue
add_to_dict(words, 'glove.6B.50d.txt')

!pip install emoji

import nltk
import re
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
import emoji
nltk.download('wordnet') # for nlp tasks
#Preporcess, tokenize and create embeddings
# Load the NLTK tokenizer
tokenizer =  TweetTokenizer()  #nltk.RegexpTokenizer(r"\w+")
lemmatizer = WordNetLemmatizer()

# Define URL, hashtag, and mention regex patterns
url_regex = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
hashtag_regex = re.compile(r'#\S+')
mention_regex = re.compile(r'@\w+')

def preprocess_type_II(tweet_text):
    if pd.isna(tweet_text) or tweet_text == '':
        return tweet_text

    # Convert urls to HTTPURL, user handles to @USER token, delete hashtags
    tweet_text = url_regex.sub('HTTPURL', tweet_text)
    tweet_text = mention_regex.sub('@USER', tweet_text)
    tweet_text = hashtag_regex.sub('', tweet_text)

    return tweet_text

def preprocess_type_III(tweet_text):
    if pd.isna(tweet_text) or tweet_text == '':
        return tweet_text

    # Use function to convert urls to HTTPURL, user handles to @USER token, delete hashtags
    tweet_text = preprocess_type_II(tweet_text)

    # Replace contractions with full forms
    contraction_mapping = {"isn't": "is not", "’cause": "because", "You'd": "you would", "I’m": "I am",
                           "Couldn't": "Could not", }
    for contraction, full_form in contraction_mapping.items():
        tweet_text = tweet_text.replace(contraction, full_form)

    # Convert emotion icons to their string text
    tweet_text = emoji.demojize(tweet_text)

    return tweet_text

def message_to_token_list(s, preprocess_func=preprocess_type_III):
    # Apply preprocessing
    s = preprocess_func(s)
    s = str(s)  # Ensure string
    s = s.replace("&amp;", "&")

    # Tokenize the preprocessed text
    tokens = tokenizer.tokenize(s)
    lowercased_tokens = [t.lower() for t in tokens]
    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]
    useful_tokens = [t for t in lemmatized_tokens if t in words]

    return useful_tokens

def message_to_word_vectors(message, word_dict=words):

    processed_list_of_tokens = message_to_token_list(message) #preprocessing and tokenization step

    vectors = [] # initialize empty array

    for token in processed_list_of_tokens:

        if token not in word_dict: #word embedding dictionary
            continue

        token_vector = word_dict[token] #whatever word we saw there grab its corresponding vector
        vectors.append(token_vector)


    return np.array(vectors, dtype=float)

#splitting the training data into train, validation and test dataframes
train_df = train_df.sample(frac=1, random_state=1)
train_df.reset_index(drop=True, inplace=True)

split_index_1 = int(len(train_df) * 0.7)
split_index_2 = int(len(train_df) * 0.85)

train_df, val_df, test_df = train_df[:split_index_1], train_df[split_index_1:split_index_2], train_df[split_index_2:]

len(train_df), len(val_df), len(test_df)

#x inputs and y is outputs
def df_to_X_y(dff):
  y = dff['sarcastic'].to_numpy().astype(int)

  all_word_vector_sequences = []#inputs

  for message in dff['tweet']:
    message_as_vector_seq = message_to_word_vectors(message)

    if message_as_vector_seq.shape[0] == 0:
      message_as_vector_seq = np.zeros(shape=(1, 50))

    all_word_vector_sequences.append(message_as_vector_seq) # diff num of tokens in each message

  return all_word_vector_sequences, y

X_train, y_train = df_to_X_y(train_df)

print(len(X_train), len(X_train[1])) #highlights different sequences length

#data analysis-use to find the max sequence length for padding purposes

sequence_lengths = []

for i in range(len(X_train)):
  sequence_lengths.append(len(X_train[i]))

import matplotlib.pyplot as plt

plt.hist(sequence_lengths)
pd.Series(sequence_lengths).describe()

#max=72
#you look for the max tokens count to make numoy array
#we need sequence length to be the same
#make all the maximum
#zero padding
from copy import deepcopy

def pad_X(X, desired_sequence_length=73):#set to max
  X_copy = deepcopy(X)

  for i, x in enumerate(X):
    x_seq_len = x.shape[0]
    sequence_length_difference = desired_sequence_length - x_seq_len
#ading the pads # 50 is the glove
    pad = np.zeros(shape=(sequence_length_difference, 50)) #50 cause glove

    X_copy[i] = np.concatenate([x, pad]) #pad zeros at end

  return np.array(X_copy).astype(float)

X_train = pad_X(X_train)
X_train.shape, y_train.shape

X_val, y_val = df_to_X_y(val_df)
X_val = pad_X(X_val)
X_val.shape, y_val.shape

X_test, y_test = df_to_X_y(test_df)
X_test = pad_X(X_test)
X_test.shape, y_test.shape

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import tensorflow as tf

#using the model
model = tf.keras.Sequential([
            # Complex model with dropout
            tf.keras.layers.Input(shape=(73, 50)),
            #tf.keras.layers.Embedding(len(tokenizer.vocab), 32),
            tf.keras.layers.Dropout(0.3),  # Add dropout after the embedding layer
            tf.keras.layers.LSTM(32),
            tf.keras.layers.Dropout(0.3),  # Add dropout after the LSTM layer
            tf.keras.layers.Dense(16, activation='relu'),
            tf.keras.layers.Dropout(0.3),  # Add dropout after the Dense layer
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

model.summary()

from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC
from tensorflow.keras.callbacks import ModelCheckpoint

cp = ModelCheckpoint('model/', save_best_only=True)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss=BinaryCrossentropy(),
              metrics=['accuracy', AUC(name='auc')])

frequencies = pd.value_counts(train_df['sarcastic'])

frequencies

#initalize weights using frequencies to help with the imbalanced dataset
#1 for non sarcastic and 4 for sarcastic- give more weight to sarcastic
weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1]}
weights

model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, callbacks=[cp], class_weight=weights)

from tensorflow.keras.models import load_model

best_model = load_model('model/')

test_predictions = (best_model.predict(X_test) > 0.5).astype(int)

from sklearn.metrics import classification_report

#for later purposes, validation is current taken from a portion of train.En

print(classification_report(y_test, test_predictions))



